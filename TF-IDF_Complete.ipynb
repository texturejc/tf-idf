{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c70ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from IPython.display import IFrame\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8548b09",
   "metadata": {},
   "source": [
    "# Topic modelling with Topic-frequency, Inverse-document-frequency (TF-IDF)\n",
    "\n",
    "While it's relatively easy to group words together, matters are less simple for documents. Nevertheless, we are usually far more interested in grouping documents together than we are words, especially when they all belong to a corpus. So an early problem that emerged in NLP centred on how we might do this. \n",
    "\n",
    "One solution proposed by Karen Spärck Jones in 1972 was TF-IDF scoring. This method is based in two ideas:\n",
    "\n",
    "* We should 'punish' words that occur frequently in all the documents in our corpus\n",
    "* We should 'reward' words that occur frequently in small numbers of documents in our corpus\n",
    "\n",
    "The intuition behind these ideas is fairly simple. For a topic to be coherent, it will generally consist of a small number of concepts. As these concepts will expressed as words, we should expect these topic-related words to concentrate in the documents that belong to this topic. However, some words (like prepositions and common verbs) will appear in all documents. So if we can can create a word-document matrix that assigns a score to each word in each document, we can gain a numerical representation––a vector––of how each document in a corpus differs from every other one.\n",
    "\n",
    "Once we have these vectors, we can then perform various operations (like clustering) upon them to discover how they might be grouped together. But how do we calculate them?\n",
    "\n",
    "The TF-IDF score for a word is the product of two quantities: the term frequency, and the inverse document frequency.\n",
    "\n",
    "$$TF\\text{-}IDF (t, d, D) = tf(t,d)\\times idf(t, D)$$\n",
    "\n",
    "Where $t$ is a term, $d$ is a document, and $D$ is the corpus. Let's create a toy corpus to illustrate this.\n",
    "\n",
    "$D$ = {{$d_1$: Atomic Burger makes a tasty burger}, {$d_2$: An atomic clock is accurate}, {$d_3$: Atomic weapons are destructive}}\n",
    "\n",
    "First, let's look at how we might calculate $tf(t,d)$. This is the relative frequency of each term in each document. That is, it's the number of times a term $t_i$ occurs in a document $d$ divided by the total number of terms in the document:\n",
    "\n",
    "$$tf(t,d) = \\frac{f_{t_i,d}}{\\sum\\limits_{t\\in d}f_{t,d}}$$\n",
    "\n",
    "For example, in document $d_1$ 'burger' has a $tf(t,d)$ score of 0.333 because there are six words and it occurs twice. Every other word has a score of 0.16 because it only occurs once. Words that occur often are therefore 'rewarded' in this part of the calculation.\n",
    "\n",
    "Now, let's look at $idf(t,D)$. This adjusts the $tf(t,d)$ score by capturing how often a word occurs across the entire corpus of documents. It is the logarithm of the number of documents in a corpus divided by the number of douments that contain the term in the corpus. \n",
    "\n",
    "$$idf(t,D) = \\log{\\frac{N}{|\\{d\\in D: t \\in d\\}|}}$$\n",
    "\n",
    "What's happening here? If a term occurs in all documents, the formula outputs a value of 0, because the total number of documents divided by the number of dcouments containing the term is $\\log(1)= 0$. If the term occurs in a smaller number of documents, the formula gives a larger number, with the largest number being given when the term only occurs in one document. For example, 'atomic' occurs in all documents, so the $idf(t,D)$ value for 'atomic' is 0. However, 'burger' occurs in only one document, so the value is $\\log{\\frac{3}{1}} = 1.09$. In this way, the $idf(t,D)$ 'punishes' words that are common and therefore not topic specific. \n",
    "\n",
    "The result is that by multiplying $tf(t,d)$ and $idf(t,D)$, we are able to capture the role played by a word in determining the topic of a document relative to a corpus. The $TF\\text{-}IDF (t, d, D)$ representation of a document is a vector of the values taken by all the words in that document relative to all the words in the corpus (with a value of 0 being taken when a word does not appear in the document).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input = 'content', strip_accents = 'ascii', stop_words = 'english')\n",
    "\n",
    "D = ['Atomic Burger makes a tasty burger', 'An atomic clock is accurate', 'Atomic weapons are destructive']\n",
    "\n",
    "v = vectorizer.fit_transform(D)\n",
    "v = v.todense().tolist()\n",
    "\n",
    "d = pd.DataFrame(\n",
    "    v,columns=vectorizer.get_feature_names_out())\n",
    "d.index = ['d1', 'd2', 'd3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5864357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba952c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [[] for i in range(len(d))]\n",
    "\n",
    "for i in range(len(d)):\n",
    "    for j in range(len(d)):\n",
    "        distances[i].append(distance.cosine(d.iloc[i], d.iloc[j]))\n",
    "        \n",
    "dist_df = pd.DataFrame(distances, columns = d.index, index = d.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a70e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bed276",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('/Users/jamescarney/Downloads/ED_twitter_data.xlsx', index_col = 0)\n",
    "\n",
    "data = data.drop_duplicates(subset = ['tweet']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [i for i in data['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc374ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b6141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectors.todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f43652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    vectors,columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ae4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca_1 = PCA(n_components = 3)\n",
    "comps_1 = pca_1.fit_transform(df)\n",
    "pc_df_1 = pd.DataFrame(data = comps_1, columns = ['PC'+str(i) for i in range(1, comps_1.shape[1]+1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters = 5, metric = 'euclidean', linkage = 'ward').fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22143255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df, pc_df_1], axis = 1)\n",
    "df_all['clusters_ag'] = [str(i) for i in clustering.labels_]\n",
    "df_all['clusters_knn'] = [str(i) for i in kmeans.labels_]\n",
    "df_all['tweet_text'] = data['tweet']\n",
    "df_all['attitude'] = data['attitude']\n",
    "df_all['likes'] = [(i+1)*100 for i in data['nlikes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2beea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_all, x='PC1', y='PC2', z='PC3',\n",
    "              color='clusters_ag', hover_data = ['tweet_text', 'attitude'])\n",
    "\n",
    "fig.update_traces(marker=dict(size = 5, line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig.show('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df_all.loc[df_all['clusters_ag'] == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358ab41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39893350",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_t = [i for i in d['tweet_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c260e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f368750",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9473d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = pd.read_pickle('/Users/jamescarney/Dropbox/textureAI/client_projects/UK_gov_corona/data/MARCH/19_MAR_window.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = gov.dropna(subset = 'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = gov.head(12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8435c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00426a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [i for i in gov['proc']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7bf7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_ = TfidfVectorizer(input = 'content', lowercase = False, tokenizer = identity_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_ = vectorizer_.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb0d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_ = vectors_.todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88362a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame(\n",
    "    vectors_, columns=vectorizer_.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472dfcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1 = PCA(n_components = 3)\n",
    "comps_1 = pca_1.fit_transform(df_)\n",
    "pc_df_1 = pd.DataFrame(data = comps_1, columns = ['PC'+str(i) for i in range(1, comps_1.shape[1]+1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d88fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62311b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.concat([df_, pc_df_1], axis = 1)\n",
    "df_['clusters_knn'] = [str(i) for i in kmeans.labels_]\n",
    "df_['tweet_text'] = gov['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d362be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df_, x='PC1', y='PC2', z='PC3',\n",
    "              color='clusters_knn', hover_data = ['tweet_text'])\n",
    "\n",
    "fig.update_traces(marker=dict(size = 5, line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig.show('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35418108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
